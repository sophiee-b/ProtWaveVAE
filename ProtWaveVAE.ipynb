{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f807d610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'ProtWaveVAE_model'...\n",
      "remote: Enumerating objects: 47, done.\u001b[K\n",
      "remote: Counting objects: 100% (47/47), done.\u001b[K\n",
      "remote: Compressing objects: 100% (38/38), done.\u001b[K\n",
      "remote: Total 47 (delta 20), reused 33 (delta 9), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (47/47), 1.81 MiB | 19.47 MiB/s, done.\n",
      "Resolving deltas: 100% (20/20), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/PraljakReps/ProtWaveVAE_model.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fc8fe59",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ProtWaveVAE_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e8ab3f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/storage/ice1/6/9/khari8/ProtWaveVAE_model'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"ProtWaveVAE_model\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cdc1e268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor and size: tensor([[[1., 0., 0.],\n",
      "         [0., 0., 1.],\n",
      "         [0., 1., 0.],\n",
      "         [1., 0., 0.]]]) torch.Size([1, 4, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from ProtWave_VAE import model_components, wavenet_decoder, model_ensemble\n",
    "\n",
    "X = torch.tensor([0, 2, 1, 0]).unsqueeze(0)\n",
    "# get the number of unique cats\n",
    "num_categories = len(torch.unique(X))\n",
    "x_one_hot = torch.nn.functional.one_hot(X, num_categories).float()\n",
    "print('Input tensor and size:', x_one_hot, x_one_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebde5892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder hyperparameters\n",
    "batch_size = x_one_hot.shape[0] # batch size\n",
    "seq_len = x_one_hot.shape[1] # length of the input sequence\n",
    "class_labels = num_categories # numer of categorical labels\n",
    "z_dim = 3 # latent space size\n",
    "C_in = class_labels\n",
    "C_out = 128 # convolution layer hidden kernel number\n",
    "kernel_size = 3 # kernel size for encoder\n",
    "num_fc = 2 # fully connect layers before embedding latent codes\n",
    "num_rates = 0 # depth of the encoder convolutions (set to 0 for max depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fece68f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder comp: q(z|x)\n",
    "encoder = model_components.GatedCNN_encoder(\n",
    "        protein_len=seq_len,\n",
    "        class_labels=class_labels,\n",
    "        z_dim=z_dim,\n",
    "        num_rates=num_rates,\n",
    "        alpha=0.1, # leaky ReLU hparam\n",
    "        kernel=kernel_size,\n",
    "        num_fc=num_fc,\n",
    "        C_in=C_in,\n",
    "        C_out=C_out\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d8f1c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wavenet decoder hyperparameters\n",
    "device = 'cpu' # device = 'cuda' # if GPU is available\n",
    "whs = 32 # dilated convolution kernel number\n",
    "hhs = 256 # top model hidden representation size\n",
    "dec_kernel_size = 3\n",
    "ndr = 5 # number of dilations (i.e. wavenet depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "537266ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder comp: p(x|z)\n",
    "decoder_xr = wavenet_decoder.Wave_generator(\n",
    "                        protein_len=seq_len,\n",
    "                        class_labels=class_labels,\n",
    "                        DEVICE=device,\n",
    "                        wave_hidden_state=whs,\n",
    "                        head_hidden_state=hhs,\n",
    "                        num_dil_rates=ndr,\n",
    "                        kernel_size=dec_kernel_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "872ec82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# latent scaling: z -> Z\n",
    "latent_upscaler = wavenet_decoder.CondNet(\n",
    "        z_dim=z_dim,\n",
    "        output_shape=(1,seq_len)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1bccc6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "ProtWaveVAE_model = model_ensemble.ProtWaveVAE(\n",
    "        DEVICE=device,\n",
    "        encoder=encoder,\n",
    "        decoder_recon=decoder_xr,\n",
    "        cond_mapper=latent_upscaler,\n",
    "        z_dim=z_dim\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c43a737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted logits and size: tensor([[[-0.0186, -0.1138,  0.0021],\n",
      "         [ 0.1117,  0.0497, -0.2652],\n",
      "         [ 0.0785, -0.0719, -0.3013],\n",
      "         [ 0.2561, -0.0243, -0.5685]]]) torch.Size([1, 4, 3])\n",
      "Inferred latent embeddings and size: tensor([[ 0.1000,  0.2000, -0.4889]]) torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "ProtWaveVAE_model.eval()\n",
    "with torch.no_grad():\n",
    "    logits_xrc, z, z_mu, z_var = ProtWaveVAE_model(x=x_one_hot)\n",
    "\n",
    "print('Predicted logits and size:', logits_xrc, logits_xrc.shape)\n",
    "print('Inferred latent embeddings and size:', z, z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa56a701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next steps: train model on their data\n",
    "# train model on new data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
